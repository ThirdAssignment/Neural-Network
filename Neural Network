_author__ = 'Xabier'

import numpy as np
#from scipy import optimize
import matplotlib.pylab as plt


X1 = np.array(([-4.5316], [6.69138], [6.01159], [-1.51829], [0.381849], [-3.69688], [-8.963], [-7.96203], [-6.43014], [-8.58233]), dtype=float)

X = np.array(([-4.5316,1], [6.69138,1], [6.01159,1], [-1.51829,1], [0.381849,1], [-3.69688,1], [-8.963,1], [-7.96203,1], [-6.43014,1], [-8.58233,1]), dtype=float)
y = np.array(([-0.228174], [0.128751], [-0.0582842], [0.672034], [0.968582], [-0.167464], [0.00629585], [0.163437], [0.0195345], [0.0561658]), dtype=float)


class Neural_Network(object):
    def __init__(self):
        #Define Hyperparameters
        self.inputLayerSize = 2
        self.outputLayerSize = 1
        self.hiddenLayerSize = 3

        #Weights (parameters)
        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)
        self.W2 = np.random.randn(self.hiddenLayerSize+1,self.outputLayerSize)

    def update(self,listW1,listW2):
        self.W1=listW1
        self.W2=listW2

    def sigmoid(self, z):
        #Apply sigmoid activation function to scalar, vector, or matrix
        return z/(1.0+abs(z))

    def forward(self, X):
        #Propogate inputs though network
        a= np.array(np.ones((10,1)))
        self.z2 = np.dot(X, self.W1)
        self.z2= np.append(self.z2,a, axis=1)
        self.a2 = self.sigmoid(self.z2)
        self.z3 = np.dot(self.a2, self.W2)
        yHat = self.sigmoid(self.z3)

        return yHat

    def sigmoidPrime(self,z):
        return 1/((1.0+abs(z))**2)

    def costFunction(self, X, y):
        #Compute cost for given X,y, use weights already stored in class.
        self.yHat = self.forward(X)
        J = 0.1*(sum((y-self.yHat)**2))
        return J

    def costFunctionPrime(self, X, y):
        #Compute derivative with respect to W and W2 for a given X and y:
        self.yHat = self.forward(X)

        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))
        dJdW2 = np.dot(self.a2.T, delta3)

        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)
        dJdW1 = np.dot(X.T, delta2)

        return dJdW1, dJdW2


    #Helper Functions for interacting with other classes:
    def getParams(self):
        #Get W1 and W2 unrolled into vector:
        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))
        return params

    def setParams(self, params):
        #Set W1 and W2 using single paramater vector.
        W1_start = 0
        W1_end = self.hiddenLayerSize * self.inputLayerSize
        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))
        W2_end = W1_end + self.hiddenLayerSize+1*self.outputLayerSize
        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize+1, self.outputLayerSize))
        return self.W1, self.W2

    def computeGradients(self, X, y):  # Same as cosFunctionPrime
        dJdW1, dJdW2 = self.costFunctionPrime(X, y)
        return dJdW1, dJdW2
        #return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))
    def gradient_des(self):
            #Learning rate
            nu=0.005
            deltaW1,deltaW2=self.computeGradients(X,y)
            self.newW1=np.array(np.zeros((self.inputLayerSize,self.hiddenLayerSize)))
            self.newW2=np.array(np.zeros((self.hiddenLayerSize+1,self.outputLayerSize)))
            for i in range(0,len(self.W1)):
                for j in range(0,len(self.W1[i])):
                    increase=-nu*deltaW1[i][j]
                    self.newW1[i][j]=self.W1[i][j]+(increase)
            for i in range(0,len(self.W2)):
                for j in range(0,len(self.W2[i])):
                    increase=-nu*deltaW2[i][j]
                    self.newW2[i][j]=self.W2[i][j]+(increase)

            return self.newW1,self.newW2

    def training(self,iter):
        list = []
        for i in range(0,iter):
            list.append(self.costFunction(X, y))
            ar1,ar2=self.gradient_des()
            self.update(ar1,ar2)
        return list


NN = Neural_Network()

NN.training(40000)
#print len(QWERTY)
# plt.scatter(range(0, 200, 1), errorr)
plt.scatter(X1,y)
plt.scatter(X1,NN.forward(X), color="red")


# plt.scatter(range(0, 200, 1), errorr)
# plt.scatter(X1,y)
#plt.plot(range(0,10000,1),QWERTY, color = "red")
_author__ = 'Xabier'

import numpy as np
#from scipy import optimize
import matplotlib.pylab as plt


X1 = np.array(([-4.5316], [6.69138], [6.01159], [-1.51829], [0.381849], [-3.69688], [-8.963], [-7.96203], [-6.43014], [-8.58233]), dtype=float)

X = np.array(([-4.5316,1], [6.69138,1], [6.01159,1], [-1.51829,1], [0.381849,1], [-3.69688,1], [-8.963,1], [-7.96203,1], [-6.43014,1], [-8.58233,1]), dtype=float)
y = np.array(([-0.228174], [0.128751], [-0.0582842], [0.672034], [0.968582], [-0.167464], [0.00629585], [0.163437], [0.0195345], [0.0561658]), dtype=float)


class Neural_Network(object):
    def __init__(self):
        #Define Hyperparameters
        self.inputLayerSize = 2
        self.outputLayerSize = 1
        self.hiddenLayerSize = 3

        #Weights (parameters)
        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)
        self.W2 = np.random.randn(self.hiddenLayerSize+1,self.outputLayerSize)

    def update(self,listW1,listW2):
        self.W1=listW1
        self.W2=listW2

    def sigmoid(self, z):
        #Apply sigmoid activation function to scalar, vector, or matrix
        return z/(1.0+abs(z))

    def forward(self, X):
        #Propogate inputs though network
        a= np.array(np.ones((10,1)))
        self.z2 = np.dot(X, self.W1)
        self.z2= np.append(self.z2,a, axis=1)
        self.a2 = self.sigmoid(self.z2)
        self.z3 = np.dot(self.a2, self.W2)
        yHat = self.sigmoid(self.z3)

        return yHat

    def sigmoidPrime(self,z):
        return 1/((1.0+abs(z))**2)

    def costFunction(self, X, y):
        #Compute cost for given X,y, use weights already stored in class.
        self.yHat = self.forward(X)
        J = 0.1*(sum((y-self.yHat)**2))
        return J

    def costFunctionPrime(self, X, y):
        #Compute derivative with respect to W and W2 for a given X and y:
        self.yHat = self.forward(X)

        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))
        dJdW2 = np.dot(self.a2.T, delta3)

        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)
        dJdW1 = np.dot(X.T, delta2)

        return dJdW1, dJdW2


    #Helper Functions for interacting with other classes:
    def getParams(self):
        #Get W1 and W2 unrolled into vector:
        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))
        return params

    def setParams(self, params):
        #Set W1 and W2 using single paramater vector.
        W1_start = 0
        W1_end = self.hiddenLayerSize * self.inputLayerSize
        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))
        W2_end = W1_end + self.hiddenLayerSize+1*self.outputLayerSize
        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize+1, self.outputLayerSize))
        return self.W1, self.W2

    def computeGradients(self, X, y):  # Same as cosFunctionPrime
        dJdW1, dJdW2 = self.costFunctionPrime(X, y)
        return dJdW1, dJdW2
        #return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))
    def gradient_des(self):
            #Learning rate
            nu=0.005
            deltaW1,deltaW2=self.computeGradients(X,y)
            self.newW1=np.array(np.zeros((self.inputLayerSize,self.hiddenLayerSize)))
            self.newW2=np.array(np.zeros((self.hiddenLayerSize+1,self.outputLayerSize)))
            for i in range(0,len(self.W1)):
                for j in range(0,len(self.W1[i])):
                    increase=-nu*deltaW1[i][j]
                    self.newW1[i][j]=self.W1[i][j]+(increase)
            for i in range(0,len(self.W2)):
                for j in range(0,len(self.W2[i])):
                    increase=-nu*deltaW2[i][j]
                    self.newW2[i][j]=self.W2[i][j]+(increase)

            return self.newW1,self.newW2

    def training(self,iter):
        list = []
        for i in range(0,iter):
            list.append(self.costFunction(X, y))
            ar1,ar2=self.gradient_des()
            self.update(ar1,ar2)
        return list


NN = Neural_Network()

NN.training(40000)
#print len(QWERTY)
# plt.scatter(range(0, 200, 1), errorr)
plt.scatter(X1,y)
plt.scatter(X1,NN.forward(X), color="red")


# plt.scatter(range(0, 200, 1), errorr)
# plt.scatter(X1,y)
#plt.plot(range(0,10000,1),QWERTY, color = "red")
_author__ = 'Xabier'

import numpy as np
#from scipy import optimize
import matplotlib.pylab as plt


X1 = np.array(([-4.5316], [6.69138], [6.01159], [-1.51829], [0.381849], [-3.69688], [-8.963], [-7.96203], [-6.43014], [-8.58233]), dtype=float)

X = np.array(([-4.5316,1], [6.69138,1], [6.01159,1], [-1.51829,1], [0.381849,1], [-3.69688,1], [-8.963,1], [-7.96203,1], [-6.43014,1], [-8.58233,1]), dtype=float)
y = np.array(([-0.228174], [0.128751], [-0.0582842], [0.672034], [0.968582], [-0.167464], [0.00629585], [0.163437], [0.0195345], [0.0561658]), dtype=float)


class Neural_Network(object):
    def __init__(self):
        #Define Hyperparameters
        self.inputLayerSize = 2
        self.outputLayerSize = 1
        self.hiddenLayerSize = 3

        #Weights (parameters)
        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)
        self.W2 = np.random.randn(self.hiddenLayerSize+1,self.outputLayerSize)

    def update(self,listW1,listW2):
        self.W1=listW1
        self.W2=listW2

    def sigmoid(self, z):
        #Apply sigmoid activation function to scalar, vector, or matrix
        return z/(1.0+abs(z))

    def forward(self, X):
        #Propogate inputs though network
        a= np.array(np.ones((10,1)))
        self.z2 = np.dot(X, self.W1)
        self.z2= np.append(self.z2,a, axis=1)
        self.a2 = self.sigmoid(self.z2)
        self.z3 = np.dot(self.a2, self.W2)
        yHat = self.sigmoid(self.z3)

        return yHat

    def sigmoidPrime(self,z):
        return 1/((1.0+abs(z))**2)

    def costFunction(self, X, y):
        #Compute cost for given X,y, use weights already stored in class.
        self.yHat = self.forward(X)
        J = 0.1*(sum((y-self.yHat)**2))
        return J

    def costFunctionPrime(self, X, y):
        #Compute derivative with respect to W and W2 for a given X and y:
        self.yHat = self.forward(X)

        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))
        dJdW2 = np.dot(self.a2.T, delta3)

        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)
        dJdW1 = np.dot(X.T, delta2)

        return dJdW1, dJdW2


    #Helper Functions for interacting with other classes:
    def getParams(self):
        #Get W1 and W2 unrolled into vector:
        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))
        return params

    def setParams(self, params):
        #Set W1 and W2 using single paramater vector.
        W1_start = 0
        W1_end = self.hiddenLayerSize * self.inputLayerSize
        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))
        W2_end = W1_end + self.hiddenLayerSize+1*self.outputLayerSize
        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize+1, self.outputLayerSize))
        return self.W1, self.W2

    def computeGradients(self, X, y):  # Same as cosFunctionPrime
        dJdW1, dJdW2 = self.costFunctionPrime(X, y)
        return dJdW1, dJdW2
        #return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))
    def gradient_des(self):
            #Learning rate
            nu=0.005
            deltaW1,deltaW2=self.computeGradients(X,y)
            self.newW1=np.array(np.zeros((self.inputLayerSize,self.hiddenLayerSize)))
            self.newW2=np.array(np.zeros((self.hiddenLayerSize+1,self.outputLayerSize)))
            for i in range(0,len(self.W1)):
                for j in range(0,len(self.W1[i])):
                    increase=-nu*deltaW1[i][j]
                    self.newW1[i][j]=self.W1[i][j]+(increase)
            for i in range(0,len(self.W2)):
                for j in range(0,len(self.W2[i])):
                    increase=-nu*deltaW2[i][j]
                    self.newW2[i][j]=self.W2[i][j]+(increase)

            return self.newW1,self.newW2

    def training(self,iter):
        list = []
        for i in range(0,iter):
            list.append(self.costFunction(X, y))
            ar1,ar2=self.gradient_des()
            self.update(ar1,ar2)
        return list


NN = Neural_Network()

NN.training(40000)
#print len(QWERTY)
# plt.scatter(range(0, 200, 1), errorr)
plt.scatter(X1,y)
plt.scatter(X1,NN.forward(X), color="red")


# plt.scatter(range(0, 200, 1), errorr)
# plt.scatter(X1,y)
#plt.plot(range(0,10000,1),QWERTY, color = "red")
plt.show()